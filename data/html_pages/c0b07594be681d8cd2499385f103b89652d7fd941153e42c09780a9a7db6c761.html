
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/">

<link rel="stylesheet" href="jemdoc.css" type="text/css">
<title>Quanquan Gu</title>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-11577423-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-11577423-2');
</script>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tbody><tr valign="top">
<td id="layout-menu">
<div class="menu-category"><img class="menu" src="./image/qgu.jpg" width="100px" align="center"></div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="research.html">Research</a></div>
<div class="menu-item"><a href="publication.html">Publications</a></div>
<div class="menu-item"><a href="teaching.html">Teaching</a></div>
<div class="menu-item"><a href="talk.html">Talks</a></div> 
<div class="menu-item"><a href="service.html">Service</a></div>    
<div class="menu-item"><a href="https://www.uclaml.org">Group</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
    <h1>Quanquan Gu</h1> <p><i><a href="https://www.howtopronounce.com/quanquan">pronunciation</a></i></p>
<div id="subtitle">Associate Professor</div>
<div id="subtitle"><a href="http://www.cs.ucla.edu">Department of Computer Science</a></div>
<div id="subtitle"><a href="http://www.ucla.edu">University of California, Los Angeles</a></div>
</div>


    


    <p> I am an Associate Professor of <a href="http://www.cs.ucla.edu">Computer Science</a> at <a href="http://www.ucla.edu">UCLA</a>. I am leading the <a href="https://www.uclaml.org">UCLA Artificial General Intelligence Lab</a>. I received my Ph.D. degree in Computer Science from the University of Illinois at Urbana-Champaign in 2014. My research is in artificial intelligence and machine learning, with a focus on nonconvex optimization, deep learning, reinforcement learning, Large Language Models (LLMs), and deep generative models (e.g., diffusion models). Recently, I have been utilizing AI to enhance scientific discovery in domains such as biology, medicine, chemistry, and public health.  </p>

<p>Here is my latest <a href="./pdf/CV.pdf">CV</a>.</p>
  

<h2>News and Annoucement</h2>
    
<ul>                 
      
    <li> <p> [Nov 28, 2023] We are oragnizing the <a href="https://ai4d3.github.io">New Frontiers of AI for Drug Discovery and Development</a> at NeurIPS 2023. </p>      

      
</ul> 
    


    
    
<h2>Recent Research Highlight</h2>
    
<ul>

<li> <p> <a href="https://openreview.net/pdf?id=a3PmRgAB5T"> Self-Play Preference Optimization for Language Model Alignment
 </a>
<br> Yue Wu*, Zhiqing Sun*, Huizhuo Yuan*, Kaixuan Ji, Yiming Yang and Quanquan Gu, <i>in Proc. of the 13th International Conference on Learning Representations (ICLR)</i>, Singapore, 2025. <a href="https://arxiv.org/abs/2405.00675">[arXiv]</a></p>    
    
<li> <p> <a href="https://openreview.net/pdf?id=q3XavKPorV"> Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation 
 </a>
<br> Huizhuo Yuan*, Zixiang Chen*, Kaixuan Ji* and Quanquan Gu, <i>in Proc. of Advances in Neural Information Processing Systems (NeurIPS) 37</i>, Vancouver, Canada, 2024. <a href="https://arxiv.org/abs/2402.10210">[arXiv]</a></p>         
    
<li> <p> <a href=""> Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models</a>
<br> Zixiang Chen*, Yihe Deng*, Huizhuo Yuan*, Kaixuan Ji and Quanquan Gu, <i>in Proc. of the 41th International Conference on Machine Learning (ICML)</i>, Vienna, Austria, 2024. <a href="https://arxiv.org/abs/2401.01335">[arXiv]</a> </p>
    
<li> <p> <a href=""> Implicit Bias of Gradient Descent for Two-layer ReLU and Leaky ReLU Networks on Nearly-orthogonal Data
 </a>
<br> Yiwen Kou*, Zixiang Chen* and Quanquan Gu, <i>in Proc. of Advances in Neural Information Processing Systems (NeurIPS) 36</i>, New Orleans, LA, USA, 2023. <a href="https://arxiv.org/abs/2310.18935">[arXiv]</a></p> 
    
<li> <p> <a href=""> Why Does Sharpness-Aware Minimization Generalize Better Than SGD?
 </a>
<br> Zixiang Chen*, Junkai Zhang*, Yiwen Kou, Xiangning Chen, Cho-Jui Hsieh and Quanquan Gu, <i>in Proc. of Advances in Neural Information Processing Systems (NeurIPS) 36</i>, New Orleans, LA, USA, 2023. <a href="https://arxiv.org/abs/2310.07269">[arXiv]</a></p>    
    
<li> <p> <a href=""> Nearly Minimax Optimal Reinforcement Learning for Linear Markov Decision Processes
 </a>
<br> Jiafan He, Heyang Zhao, Dongruo Zhou and Quanquan Gu, <i>in Proc. of the 40th International Conference on Machine Learning (ICML)</i>, Hawaii, USA, 2023. <a href="https://arxiv.org/abs/2212.06132">[arXiv]</a> </p>    

<li> <p> <a href=""> Benign Overfitting for Two-layer ReLU Convolutional Neural Networks
 </a>
<br> Yiwen Kou*, Zixiang Chen*, Yuanzhou Chen and Quanquan Gu, <i>in Proc. of the 40 th International Conference on Machine Learning (ICML)</i>, Hawaii, USA, 2023. <a href="https://arxiv.org/abs/2303.04145">[arXiv]</a> </p>    
    
<li> <p> <a href=""> Variance-Dependent Regret Bounds for Linear Bandits and Reinforcement
 Learning: Adaptivity and Computational Efficiency
 </a>
<br> Heyang Zhao, Jiafan He, Dongruo Zhou, Tong Zhang and Quanquan Gu, <i>in Proc. of the 36th Annual Conference on Learning Theory (COLT)</i>, Bangalore, India, 2023. <a href="https://arxiv.org/abs/2302.10371">[arXiv]</a> </p> 
    
<li> <p> <a href=""> Computationally Efficient Horizon-Free Reinforcement Learning for Linear Mixture MDPs
 </a>
<br> Dongruo Zhou and Quanquan Gu, <i>in Proc. of Advances in Neural Information Processing Systems (NeurIPS) 35</i>, New Orleans, LA, USA, 2022. <font color="red">Oral Presentation</font> <a href="https://arxiv.org/abs/2205.11507">[arXiv]</a></p>                 
    
<li> <p> <a href=""> Nearly Optimal Algorithms for Linear Contextual Bandits with Adversarial
 Corruptions
 </a>
<br> Jiafan He, Dongruo Zhou, Tong Zhang and Quanquan Gu, <i>in Proc. of Advances in Neural Information Processing Systems (NeurIPS) 35</i>, New Orleans, LA, USA, 2022. <a href="http://arxiv.org/abs/2205.06811">[arXiv]</a></p>     
    
<li> <p> <a href=""> Risk Bounds of Multi-Pass SGD for Least Squares in the Interpolation
 Regime
 </a>
<br> Difan Zou*, Jingfeng Wu*, Vladimir Braverman, Quanquan Gu and Sham M. Kakade, <i>in Proc. of Advances in Neural Information Processing Systems (NeurIPS) 35</i>, New Orleans, LA, USA, 2022. <a href="https://arxiv.org/abs/2203.03159">[arXiv]</a></p>          
    
<li> <p> <a href=""> Benign Overfitting in Two-layer Convolutional Neural Networks
 </a>
<br> Yuan Cao*, Zixiang Chen*, Mikhail Belkin and Quanquan Gu, <i>in Proc. of Advances in Neural Information Processing Systems (NeurIPS) 35</i>, New Orleans, LA, USA, 2022. <font color="red">Oral Presentation</font> <a href="https://arxiv.org/abs/2202.06526">[arXiv]</a></p>   
    
<li> <p> <a href=""> Last Iterate Risk Bounds of SGD with Decaying Stepsize for
 Overparameterized Linear Regression
 </a>
<br> Jingfeng Wu*, Difan Zou*, Vladimir Braverman, Quanquan Gu and Sham M. Kakade, <i>in Proc. of the 39th International Conference on Machine Learning (ICML)</i>, Baltimore, MD, USA, 2022. <font color="red">Long presentation</font> <a href="http://arxiv.org/abs/2110.06198">[arXiv]</a> </p>      
    
<li> <p> <a href=""> The Benefits of Implicit Regularization from SGD in Least Squares Problems
 </a>
<br> Difan Zou*, Jingfeng Wu*, Vladimir Braverman, Quanquan Gu, Dean P. Foster and Sham M. Kakade, <i>in Proc. of Advances in Neural Information Processing Systems (NeurIPS) 34</i>, 2021. <a href="https://arxiv.org/abs/2108.04552">[arXiv]</a></p>        
    
<li> <p> <a href="">
Risk Bounds for Over-parameterized Maximum Margin Classification on Sub-Gaussian Mixtures
 </a>
<br> Yuan Cao, Quanquan Gu, Mikhail Belkin, <i>in Proc. of Advances in Neural Information Processing Systems (NeurIPS) 34</i>, 2021. <a href="http://arxiv.org/abs/2104.13628">[arXiv]</a></p>    
    
<li> <p> <a href="">
Benign Overfitting of Constant-Stepsize SGD for Linear Regression
 </a>
<br> Difan Zou, Jingfeng Wu, Vladimir Braverman, Quanquan Gu and Sham M. Kakade, <i>in Proc. of the 34th Annual Conference on Learning Theory (COLT)</i>, 2021. <a href="https://arxiv.org/abs/2103.12692">[arXiv]</a></p>     
    
<li> <p> <a href="">
Nearly Minimax Optimal Reinforcement Learning for Linear Mixture Markov Decision Processes
 </a>
<br> Dongruo Zhou, Quanquan Gu and Csaba Szepesvári, <i>in Proc. of the 34th Annual Conference on Learning Theory (COLT)</i>, 2021. <a href="https://arxiv.org/abs/2012.08507">[arXiv]</a></p>    
    
<li> <p> <a href="Agnostic Learning of Halfspaces with Gradient Descent via Soft Margins"> Agnostic Learning of Halfspaces with Gradient Descent via Soft Margins</a>
<br> Spencer Frei, Yuan Cao and Quanquan Gu, <i>in Proc. of the 38th International Conference on Machine Learning (ICML)</i>, 2021. <font color="red">Long talk</font> <a href="https://arxiv.org/abs/2010.00539">[arXiv] </a></p>     
    
<li> <p> <a href="http://proceedings.mlr.press/v139/zhou21a.html"> Provably Efficient Reinforcement Learning for Discounted MDPs with
 Feature Mapping
 </a>
<br> Dongruo Zhou, Jiafan He and Quanquan Gu, <i>in Proc. of the 38th International Conference on Machine Learning (ICML)</i>, 2021. <a href="https://arxiv.org/abs/2006.13165">[arXiv]</a></p>     
    
<li> <p> <a href="https://openreview.net/forum?id=fgd7we_uZa6"> How Much Over-parameterization Is Sufficient to Learn Deep ReLU
 Networks? </a>
<br> Zixiang Chen*, Yuan Cao*, Difan Zou* and Quanquan Gu, <i>in Proc. of the 9th International Conference on Learning Representations (ICLR)</i>, 2021. <a href="https://arxiv.org/abs/1911.12360">[arXiv]</a></p>       

<li> <p> <a href=""> A Generalized Neural Tangent Kernel Analysis for Two-layer Neural Networks
 </a>
<br> Zixiang Chen, Yuan Cao, Quanquan Gu and Tong Zhang, <i>in Proc. of Advances in Neural Information Processing Systems (NeurIPS) 33</i>, 2020. <a href="https://arxiv.org/abs/2002.04026">[arXiv]</a></p>     
    
<li> <p> <a href=""> 
Generalization Bounds of Stochastic Gradient Descent for Wide and Deep Neural Networks</a>
<br> Yuan Cao and Quanquan Gu, <i>in Proc. of Advances in Neural Information Processing Systems (NeurIPS) 32, Vancouver, Canada</i>, 2019. <font color="red">Spotlight presentation</font> <a href="https://arxiv.org/pdf/1905.13210.pdf">[arXiv]</a></p>    
    
<li> <p> <a href=""> 
An Improved Analysis of Training Over-parameterized Deep Neural Networks</a>
<br> Difan Zou and Quanquan Gu, <i>in Proc. of Advances in Neural Information Processing Systems (NeurIPS) 32, Vancouver, Canada</i>, 2019. <a href="https://arxiv.org/pdf/1906.04688">[arXiv]</a></p>
    
<li> <p> <a href=""> 
Gradient Descent Optimizes Over-parameterized Deep ReLU
 Networks </a>
    <br> Difan Zou*, Yuan Cao*, Dongruo Zhou and Quanquan Gu, <i>Accepted by the Machine Learning Journal (MLJ)</i>, 2019. <a href="https://arxiv.org/pdf/1811.08888.pdf">[arXiv]</a></p>            
    
<li> <p> <a href=""> 
Lower Bounds for Smooth Nonconvex Finite-Sum Optimization </a>
<br> Dongruo Zhou and Quanquan Gu, <i>in Proc. of the 36th International Conference on Machine Learning (ICML), Long Beach, CA, USA</i>, 2019. <a href="https://arxiv.org/abs/1901.11224">[arXiv]</a> </p>       


    
<li> <p> <a href="./pdf/SNVRG.pdf"> Stochastic Nested Variance Reduction for Nonconvex Optimization </a>
<br> Dongruo Zhou, Pan Xu and Quanquan Gu, <i>In Proc. of Advances in Neural Information Processing Systems (NeurIPS) 31, Montréal, Canada</i>, 2018. <font color="red">Spotlight presentation</font> <a href="http://arxiv.org/abs/1806.07811">[arXiv]</a> </p>        
    

    
</ul>
    
<h2>Recent Services</h2>    

    <ul> 
        
        <li> <p>Senior Area Chair for NeurIPS 2023, Area Chair for ICLR 2024, AISTATS 2024, AAAI 2024</p>  
    </ul>     
    
<h2>Awards</h2>    

    <ul>
        <li> <p>WSDM Test of Time Paper Award, 2024</p>
        <li> <p>Alfred P. Sloan Research Fellowship, 2022</p>           
        <li> <p>JP Morgan Faculty Research Award, 2022</p>
        <li> <p>IJCAI Early Career Spotlight, 2020</p>
        <li> <p>AWS Machine Learning Research Award, 2019</p>
        <li> <p>Simons Berkeley Research Fellowship, 2019</p>
        <li> <p>Adobe Data Science Research Award, 2018</p>
        <li> <p>Salesforce Deep Learning Research Award, 2018</p>
        <li> <p>NSF CAREER Award, 2017</p>    
        <li> <p>Yahoo! Academic Career Enhancement Award, 2015</p>  
    </ul>             

<h2>Contact</h2>
<ul>
<li><p>Address: EVI 491A, 404 Westwood Plaza, Los Angeles, CA 90095</p>
</li>
<li><p>Email: qgu at cs dot ucla dot edu</p>
</li>
    
    
<a class="twitter-timeline" data-width="400" data-height="800" href="https://twitter.com/QuanquanGu?ref_src=twsrc%5Etfw">Tweets by QuanquanGu</a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
    
</ul>
    

<p><br></p>
<div id="footer">
<div id="footer-text">
Page generated by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</tbody></table>


</body></html>